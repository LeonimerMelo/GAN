{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LeonimerMelo/GANs/blob/main/CIFAR10_DCGAN_v6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#CIFAR10 DCGAN Example\n",
        "\n",
        "Generative Adversarial Networks, or GANs, are an architecture for training generative models, such as deep convolutional neural networks for generating images.\n",
        "\n",
        "Developing a GAN for generating images requires both a discriminator convolutional neural network model for classifying whether a given image is real or generated and a generator model that uses inverse convolutional layers to transform an input to a full two-dimensional image of pixel values.\n",
        "\n",
        "It can be challenging to understand both how GANs work and how deep convolutional neural network models can be trained in a GAN architecture for image generation. A good starting point for beginners is to practice developing and using GANs on standard image datasets used in the field of computer vision, such as the CIFAR small object photograph dataset. Using small and well-understood datasets means that smaller models can be developed and trained quickly, allowing focus to be put on the model architecture and image generation process itself."
      ],
      "metadata": {
        "id": "ePScIS3En0QC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center><img src=\"https://miro.medium.com/max/1280/1*Mw2c3eY5khtXafe5W-Ms_w.jpeg\" width=800></center>"
      ],
      "metadata": {
        "id": "NCkt3f1J2t4K"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Fkd1smHNlnJ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from numpy import zeros\n",
        "from numpy import ones\n",
        "from numpy.random import randn\n",
        "from numpy.random import randint\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Reshape\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.layers import Conv2D\n",
        "from tensorflow.keras.layers import Conv2DTranspose\n",
        "from tensorflow.keras.layers import LeakyReLU\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from matplotlib import pyplot as plt\n",
        "from keras.models import load_model\n",
        "from keras.models import model_from_json\n",
        "from keras import backend\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MsA4NhWtx0uQ",
        "outputId": "2150e2b3-2913-4e94-aa8a-89cd9e3dea58"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##CIFAR-10 Small Object Photograph Dataset\n",
        "CIFAR is an acronym that stands for the Canadian Institute For Advanced Research and the CIFAR-10 dataset was developed along with the CIFAR-100 dataset (covered in the next section) by researchers at the CIFAR institute.\n",
        "\n",
        "The dataset is comprised of 60,000 32×32 pixel color photographs of objects from 10 classes, such as frogs, birds, cats, ships, airplanes, etc.\n",
        "\n",
        "These are very small images, much smaller than a typical photograph, and the dataset is intended for computer vision research.\n",
        "\n",
        "Keras provides access to the CIFAR10 dataset via the `cifar10.load_dataset()` function. It returns two tuples, one with the input and output elements for the standard training dataset, and another with the input and output elements for the standard test dataset."
      ],
      "metadata": {
        "id": "2411TCFJp0E0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oEBO_iVuloxK"
      },
      "outputs": [],
      "source": [
        "# Function to visualize random Training Images\n",
        "def show_dataset(X_train, y_train):\n",
        "  # define text labels (source: https://www.cs.toronto.edu/~kriz/cifar.html)\n",
        "  cifar10_labels = ['airplane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
        "  j = np.random.randint(len(X_train)-60)\n",
        "  fig = plt.figure(figsize=(10,7))\n",
        "  for i in range(60):\n",
        "    ax = fig.add_subplot(6, 10, i + 1, xticks=[], yticks=[])\n",
        "    ax.imshow(X_train[i+j])\n",
        "    #ax.set_title(\"{} {}\".format(cifar10_labels[y_train[i+j][0]], str(y_train[i+j])), size=7)\n",
        "    ax.set_title(\"{}\".format(cifar10_labels[int(y_train[i+j][0])]), size=7)\n",
        "    plt.axis('off')\n",
        "  #plt.tight_layout()\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
        "show_dataset(X_train, y_train)"
      ],
      "metadata": {
        "id": "cYiCMAKObfFy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('X_train shape:', X_train.shape)\n",
        "print('X_test shape:', X_test.shape)\n",
        "print(X_train.min(), X_train.max())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ohS5KKH9qOXe",
        "outputId": "0a09ceae-c13c-4b04-a368-d3aa80094b61"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train shape: (50000, 32, 32, 3)\n",
            "X_test shape: (10000, 32, 32, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will use the images in the training dataset as the basis for training a Generative Adversarial Network.\n",
        "\n",
        "Specifically, the generator model will learn how to generate new plausible photographs of objects using a discriminator that will try and distinguish between real images from the CIFAR10 training dataset and new images output by the generator model.\n",
        "\n",
        "This is a non-trivial problem that requires modest generator and discriminator models that are probably most effectively trained on GPU hardware."
      ],
      "metadata": {
        "id": "HcCd0vwDbisl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JyGzKkpAxlYE"
      },
      "outputs": [],
      "source": [
        "# Rotina para salvar o modelo da rede treinada (modelo, pesos e bias) para predições futuras sem ter que treiná-lo novamente\n",
        "def model_save(model, name):\n",
        "  mod = str(name)\n",
        "  model_json = model.to_json()\n",
        "  try:\n",
        "    with open('/content/drive/MyDrive/Colab Notebooks/temp/model'+ mod +'.json','w') as json_file:\n",
        "        json_file.write(model_json)\n",
        "    model.save_weights('/content/drive/MyDrive/Colab Notebooks/temp/weights'+ mod + '.h5')\n",
        "  except:\n",
        "    print('drive canot be open to save files!')\n",
        "    return\n",
        "\n",
        "  print('Saved model ' + mod + ' to disk')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##The discriminator model\n",
        "The discriminator model must take a sample image from our dataset as input and output a classification prediction as to whether the sample is real or fake. This is a binary classification problem.\n",
        "\n",
        "- **Inputs**: Image with three color channel and 32×32 pixels in size.\n",
        "- **Outputs**: Binary classification, likelihood the sample is real (or fake).\n",
        "\n",
        "The discriminator model has a normal convolutional layer followed by three convolutional layers using a stride of 2×2 to downsample the input image. The model has no pooling layers and a single node in the output layer with the sigmoid activation function to predict whether the input sample is real or fake. The model is trained to minimize the binary cross entropy loss function, appropriate for binary classification.\n",
        "\n",
        "We will use some best practices in defining the discriminator model, such as the use of LeakyReLU instead of ReLU, using Dropout, and using the Adam version of stochastic gradient descent with a learning rate of 0.0002 and a momentum of 0.5."
      ],
      "metadata": {
        "id": "UwhMH_PvcC0S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center><img src=\"https://content.iospress.com/media/ifs/2021/41-2/ifs-41-2-ifs210843/ifs-41-ifs210843-g001.jpg?width=600\"></center>"
      ],
      "metadata": {
        "id": "O64-hlW74de8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you are creating many models in a loop, this global state will consume an increasing amount of memory over time, and you may want to clear it. Calling `clear_session()` releases the global state: this helps avoid clutter from old models and layers, especially when memory is limited."
      ],
      "metadata": {
        "id": "ojq_uA6VpfCV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Resets all state generated by Keras\n",
        "backend.clear_session()"
      ],
      "metadata": {
        "id": "q7YC3TnUXx11"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define the standalone discriminator model\n",
        "def define_discriminator(in_shape=(32, 32, 3)):\n",
        "    model = Sequential(name='Discriminator')\n",
        "    # normal\n",
        "    model.add(Conv2D(64, (3, 3), padding='same', input_shape=in_shape))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    # downsample\n",
        "    model.add(Conv2D(128, (3, 3), strides=(2, 2), padding='same'))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    # downsample\n",
        "    model.add(Conv2D(128, (3, 3), strides=(2, 2), padding='same'))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    # downsample\n",
        "    model.add(Conv2D(256, (3, 3), strides=(2, 2), padding='same'))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    # classifier\n",
        "    model.add(Flatten())\n",
        "    model.add(Dropout(0.4))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    # compile model\n",
        "    opt = Adam(learning_rate=0.0002, beta_1=0.5)\n",
        "    model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
        "    return model"
      ],
      "metadata": {
        "id": "2pFxXImPdSL5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create the discriminator\n",
        "d_model = define_discriminator()\n",
        "# summarize the model\n",
        "d_model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "woRwIgXsdaXV",
        "outputId": "73894cf1-0c4c-4f48-9d87-24f69e5ea383"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"Discriminator\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 32, 32, 64)        1792      \n",
            "                                                                 \n",
            " leaky_re_lu (LeakyReLU)     (None, 32, 32, 64)        0         \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 16, 16, 128)       73856     \n",
            "                                                                 \n",
            " leaky_re_lu_1 (LeakyReLU)   (None, 16, 16, 128)       0         \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 8, 8, 128)         147584    \n",
            "                                                                 \n",
            " leaky_re_lu_2 (LeakyReLU)   (None, 8, 8, 128)         0         \n",
            "                                                                 \n",
            " conv2d_3 (Conv2D)           (None, 4, 4, 256)         295168    \n",
            "                                                                 \n",
            " leaky_re_lu_3 (LeakyReLU)   (None, 4, 4, 256)         0         \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 4096)              0         \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 4096)              0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1)                 4097      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 522497 (1.99 MB)\n",
            "Trainable params: 522497 (1.99 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Running the example first summarizes the model architecture, showing the input and output from each layer.\n",
        "\n",
        "We can see that the aggressive 2×2 stride acts to down-sample the input image, first from 32×32 to 16×16, then to 8×8 and more before the model makes an output prediction.\n",
        "\n",
        "This pattern is by design as we do not use pooling layers and use the large stride to achieve a similar downsampling effect. We will see a similar pattern, but in reverse in the generator model."
      ],
      "metadata": {
        "id": "1RswCiMDe0Rq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Load CIFAR10 dataset\n",
        "We will use the cifar.load_data() function to load the CIFAR-10 dataset and just use the input part of the training dataset as the real images.\n",
        "\n",
        "We must scale the pixel values from the range of unsigned integers in [0,255] to the normalized range of [-1,1].\n",
        "\n",
        "The generator model will generate images with pixel values in the range [-1,1] as it will use the tanh activation function, a best practice. It is also a good practice for the real images to be scaled to the same range."
      ],
      "metadata": {
        "id": "75frfFEqfesE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load cifar10 dataset and prepare cifar10 training images\n",
        "# convert from unsigned ints to floats\n",
        "X = np.concatenate([X_train, X_test])\n",
        "X = X.astype('float32')\n",
        "#X = X_train.astype('float32')\n",
        "# scale from [0,255] to [-1,1]\n",
        "dataset = (X - 127.5) / 127.5\n",
        "print(dataset.shape)\n",
        "print(dataset.min(), dataset.max())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r_GLxQ4WfXEE",
        "outputId": "0fe5774c-a733-4fab-fbb0-00f2611f9d4c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(60000, 32, 32, 3)\n",
            "-1.0 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Real samples\n",
        "The model will be updated in batches, specifically with a collection of real samples and a collection of generated samples. On training, an epoch is defined as one pass through the entire training dataset.\n",
        "\n",
        "We could systematically enumerate all samples in the training dataset, and that is a good approach, but good training via stochastic gradient descent requires that the training dataset be shuffled prior to each epoch. A simpler approach is to select random samples of images from the training dataset.\n",
        "\n",
        "The `generate_real_samples()` function below will take the training dataset as an argument and will select a random subsample of images; it will also return class labels for the sample, specifically a class label of 1, to indicate real images."
      ],
      "metadata": {
        "id": "EO2z_ndef1L9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# select real samples\n",
        "def generate_real_samples(dataset, n_samples):\n",
        "    # choose random instances\n",
        "    ix = randint(0, dataset.shape[0], n_samples)\n",
        "    # retrieve selected images\n",
        "    X = dataset[ix]\n",
        "    # generate 'real' class labels (1)\n",
        "    y = ones((n_samples, 1))\n",
        "    return X, y"
      ],
      "metadata": {
        "id": "zaRJulQagNUE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Source of fake images\n",
        "We don’t have a generator model yet, so instead, we can generate images comprised of random pixel values, specifically random pixel values in the range [0,1], then scaled to the range [-1, 1] like our scaled real images.\n",
        "\n",
        "The `generate_fake_samples()` function below implements this behavior and generates images of random pixel values and their associated class label of 0, for fake.\n",
        "\n",
        "We can update the `generate_fake_samples()` function from the previous section to take the generator model as an argument and use it to generate the desired number of samples by first calling the `generate_latent_points()` function to generate the required number of points in latent space as input to the model."
      ],
      "metadata": {
        "id": "kA-dK2R_gcxD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# use the generator to generate n fake examples, with class labels\n",
        "def generate_fake_samples(g_model, latent_dim, n_samples):\n",
        "    # generate points in latent space\n",
        "    x_input = generate_latent_points(latent_dim, n_samples)\n",
        "    # predict outputs\n",
        "    X = g_model.predict(x_input, verbose=0)\n",
        "    # create 'fake' class labels (0)\n",
        "    y = zeros((n_samples, 1))\n",
        "    return X, y"
      ],
      "metadata": {
        "id": "uHNgtRjmgp6r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##The generator model\n",
        "The generator model is responsible for creating new, fake, but plausible small photographs of objects. It does this by taking a point from the latent space as input and outputting a square color image.\n",
        "\n",
        "The latent space is an arbitrarily defined vector space of Gaussian-distributed values, e.g. 100 dimensions. It has no meaning, but by drawing points from this space randomly and providing them to the generator model during training, the generator model will assign meaning to the latent points and, in turn, the latent space, until, at the end of training, the latent vector space represents a compressed representation of the output space, CIFAR-10 images, that only the generator knows how to turn into plausible CIFAR-10 images.\n",
        "\n",
        "- **Inputs**: Point in latent space, e.g. a 100-element vector of Gaussian random numbers.\n",
        "- **Outputs**: Two-dimensional square color image (3 channels) of 32 x 32 pixels with pixel values in [-1,1].\n",
        "\n",
        "Note: we don’t have to use a 100 element vector as input; it is a round number and widely used, but I would expect that 10, 50, or 500 would work just as well.\n",
        "\n",
        "Developing a generator model requires that we transform a vector from the latent space with, 100 dimensions to a 2D array with 32 x 32 x 3, or 3,072 values. There are a number of ways to achieve this, but there is one approach that has proven effective on deep convolutional generative adversarial networks. It involves two main elements.\n",
        "\n",
        "The first is a Dense layer as the first hidden layer that has enough nodes to represent a low-resolution version of the output image. Specifically, an image half the size (one quarter the area) of the output image would be 16x16x3, or 768 nodes, and an image one quarter the size (one eighth the area) would be 8 x 8 x 3, or 192 nodes.\n",
        "\n",
        "With some experimentation, I have found that a smaller low-resolution version of the image works better. Therefore, we will use 4 x 4 x 3, or 48 nodes.\n",
        "\n",
        "We don’t just want one low-resolution version of the image; we want many parallel versions or interpretations of the input. This is a pattern in convolutional neural networks where we have many parallel filters resulting in multiple parallel activation maps, called feature maps, with different interpretations of the input. We want the same thing in reverse: many parallel versions of our output with different learned features that can be collapsed in the output layer into a final image. The model needs space to invent, create, or generate. Therefore, the first hidden layer, the Dense, needs enough nodes for multiple versions of our output image, such as 256."
      ],
      "metadata": {
        "id": "WOBOC-3khxjy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<center><img src=\"https://pic3.zhimg.com/v2-76005a6c36fe51baa2eea5cefc8baf2e_r.jpg\" width=800></center>"
      ],
      "metadata": {
        "id": "gRnv8SBo32Ys"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define the standalone generator model\n",
        "def define_generator(latent_dim):\n",
        "    model = Sequential(name='Generator')\n",
        "    # foundation for 4x4 image\n",
        "    n_nodes = 256 * 4 * 4\n",
        "    model.add(Dense(n_nodes, input_dim=latent_dim))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    model.add(Reshape((4, 4, 256)))\n",
        "    # upsample to 8x8\n",
        "    model.add(Conv2DTranspose(128, (4, 4), strides=(2, 2), padding='same'))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    # upsample to 16x16\n",
        "    model.add(Conv2DTranspose(128, (4, 4), strides=(2, 2), padding='same'))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    # upsample to 32x32\n",
        "    model.add(Conv2DTranspose(128, (4, 4), strides=(2, 2), padding='same'))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    # output layer\n",
        "    model.add(Conv2D(3, (3, 3), activation='tanh', padding='same'))\n",
        "    return model"
      ],
      "metadata": {
        "id": "1MMWsNPNiZDy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The next major architectural innovation involves upsampling the low-resolution image to a higher resolution version of the image. There are two common ways to do this upsampling process, sometimes called deconvolution. One way is to use an `UpSampling2D` layer (like a reverse pooling layer) followed by a normal Conv2D layer. The other and perhaps more modern way is to combine these two operations into a single layer, called a `Conv2DTranspose`. We will use this latter approach for our generator.\n",
        "\n",
        "The `Conv2DTranspose` layer can be configured with a stride of (2×2) that will quadruple the area of the input feature maps (double their width and height dimensions). It is also good practice to use a kernel size that is a factor of the stride (e.g. double) to avoid a checkerboard pattern that can sometimes be observed when upsampling.\n",
        "\n",
        "Again, we will use the LeakyReLU with a default slope of 0.2, reported as a best practice when training GAN models.\n",
        "\n",
        "The output layer of the model is a Conv2D with three filters for the three required channels and a kernel size of 3×3 and ‘same‘ padding, designed to create a single feature map and preserve its dimensions at 32 x 32 x 3 pixels. A tanh activation is used to ensure output values are in the desired range of [-1,1], a current best practice."
      ],
      "metadata": {
        "id": "4AhDMFsgitp4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define the size of the latent space\n",
        "latent_dim = 100\n",
        "# define the generator model\n",
        "g_model = define_generator(latent_dim)\n",
        "# summarize the model\n",
        "g_model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eH4H-vr_jOFc",
        "outputId": "d5222340-c2ac-46e8-fb88-61f4dceaf222"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"Generator\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_1 (Dense)             (None, 4096)              413696    \n",
            "                                                                 \n",
            " leaky_re_lu_4 (LeakyReLU)   (None, 4096)              0         \n",
            "                                                                 \n",
            " reshape (Reshape)           (None, 4, 4, 256)         0         \n",
            "                                                                 \n",
            " conv2d_transpose (Conv2DTr  (None, 8, 8, 128)         524416    \n",
            " anspose)                                                        \n",
            "                                                                 \n",
            " leaky_re_lu_5 (LeakyReLU)   (None, 8, 8, 128)         0         \n",
            "                                                                 \n",
            " conv2d_transpose_1 (Conv2D  (None, 16, 16, 128)       262272    \n",
            " Transpose)                                                      \n",
            "                                                                 \n",
            " leaky_re_lu_6 (LeakyReLU)   (None, 16, 16, 128)       0         \n",
            "                                                                 \n",
            " conv2d_transpose_2 (Conv2D  (None, 32, 32, 128)       262272    \n",
            " Transpose)                                                      \n",
            "                                                                 \n",
            " leaky_re_lu_7 (LeakyReLU)   (None, 32, 32, 128)       0         \n",
            "                                                                 \n",
            " conv2d_4 (Conv2D)           (None, 32, 32, 3)         3459      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1466115 (5.59 MB)\n",
            "Trainable params: 1466115 (5.59 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Running the example summarizes the layers of the model and their output shape. We can see that, as designed, the first hidden layer has 4,096 parameters or 256 x 4 x 4, the activations of which are reshaped into 256 4 x 4 feature maps. The feature maps are then upscaled via the three Conv2DTranspose layers to the desired output shape of 32 x 32, until the output layer where three filter maps (channels) are created."
      ],
      "metadata": {
        "id": "0jqJ3nzklAsE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The first step is to generate new points in the latent space. We can achieve this by calling the randn() NumPy function for generating arrays of random numbers drawn from a standard Gaussian.\n",
        "\n",
        "The array of random numbers can then be reshaped into samples, that is n rows with 100 elements per row. The `generate_latent_points()` function below implements this and generates the desired number of points in the latent space that can be used as input to the generator model."
      ],
      "metadata": {
        "id": "dSahk5NhmPmD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# generate points in latent space as input for the generator\n",
        "def generate_latent_points(latent_dim, n_samples):\n",
        "    # generate points in the latent space\n",
        "    x_input = randn(latent_dim * n_samples)\n",
        "    # reshape into a batch of inputs for the network\n",
        "    x_input = x_input.reshape(n_samples, latent_dim)\n",
        "    return x_input"
      ],
      "metadata": {
        "id": "MK53R1AvmTyM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Train the generator model\n",
        "The weights in the generator model are updated based on the performance of the discriminator model. When the discriminator is good at detecting fake samples, the generator is updated more, and when the discriminator model is relatively poor or confused when detecting fake samples, the generator model is updated less. This defines the zero-sum or adversarial relationship between these two models.\n",
        "\n",
        "There may be many ways to implement this using the Keras API, but perhaps the simplest approach is to create a new model that combines the generator and discriminator models. Specifically, a new **GAN model** can be defined that stacks the generator and discriminator such that the generator receives as input random points in the latent space and generates samples that are fed into the discriminator model directly, classified, and the output of this larger model can be used to update the model weights of the generator.\n",
        "\n",
        "To be clear, we are not talking about a new third model, just a new logical model that uses the already-defined layers and weights from the standalone generator and discriminator models. Only the discriminator is concerned with distinguishing between real and fake examples, therefore the discriminator model can be trained in a standalone manner on examples of each.\n",
        "\n",
        "The generator model is only concerned with the discriminator’s performance on fake examples. Therefore, *we will mark all of the layers in the discriminator as not trainable when it is part of the GAN model* so that they can not be updated and overtrained on fake examples.\n",
        "\n",
        "When training the generator via this logical GAN model, there is one more important change. We want the discriminator to think that the samples output by the generator are real, not fake. *Therefore, when the generator is trained as part of the GAN model, we will mark the generated samples as real* (class 1)."
      ],
      "metadata": {
        "id": "2UE9RSrPnGKL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##GAN model\n",
        "We can imagine that the discriminator will then classify the generated samples as not real (class 0) or a low probability of being real (0.3 or 0.5). The backpropagation process used to update the model weights will see this as a large error and will update the model weights (i.e. only the weights in the generator) to correct for this error, in turn making the generator better at generating good fake samples.\n",
        "\n",
        "Let’s make this concrete.\n",
        "\n",
        "- **Inputs**: Point in latent space, e.g. a 100-element vector of Gaussian random numbers.\n",
        "- **Outputs**: Binary classification, likelihood the sample is real (or fake).\n",
        "\n",
        "The `define_gan()` function below takes as arguments the already-defined generator and discriminator models and creates the new, logical third model subsuming these two models. The weights in the discriminator are marked as not trainable, which only affects the weights as seen by the GAN model and not the standalone discriminator model.\n",
        "\n",
        "The GAN model then uses the same binary cross entropy loss function as the discriminator and the efficient Adam version of stochastic gradient descent with the learning rate of 0.0002 and momentum of 0.5, recommended when training deep convolutional GANs."
      ],
      "metadata": {
        "id": "lFVr2shoykj4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<center><img src=\"https://developer.ibm.com/developer/default/articles/generative-adversarial-networks-explained/images/GANs.jpg\" width=700></center>"
      ],
      "metadata": {
        "id": "zM--MAsS5iDq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define GAN model: the combined generator and discriminator model, for updating the generator\n",
        "def define_gan(g_model, d_model):\n",
        "  # make weights in the discriminator not trainable\n",
        "  d_model.trainable = False\n",
        "  # connect them\n",
        "  model = Sequential(name='GAN')\n",
        "  # add generator\n",
        "  model.add(g_model)\n",
        "  # add the discriminator\n",
        "  model.add(d_model)\n",
        "  # compile model\n",
        "  opt = Adam(learning_rate=0.0002, beta_1=0.5)\n",
        "  # model.compile(loss='binary_crossentropy', optimizer=opt)\n",
        "  model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
        "  #model.compiled_metrics == None\n",
        "  return model"
      ],
      "metadata": {
        "id": "PiXws4t7zOo1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Making the discriminator not trainable is a clever trick** in the Keras API. The trainable property impacts the model after it is compiled. The discriminator model was compiled with trainable layers, therefore the model weights in those layers will be updated when the standalone model is updated via calls to the `train_on_batch()` function.\n",
        "\n",
        "The discriminator model was then marked as not trainable, added to the GAN model, and compiled. In this model, the model weights of the discriminator model are not trainable and cannot be changed when the GAN model is updated via calls to the `train_on_batch()` function. This change in the trainable property does not impact the training of the standalone discriminator model."
      ],
      "metadata": {
        "id": "2xqcVUT0zVnP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create the gan\n",
        "gan_model = define_gan(g_model, d_model)\n",
        "# summarize gan model\n",
        "gan_model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vi5BUqZTzpJ3",
        "outputId": "36698526-d07f-4f59-921d-010766200b38"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"GAN\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " Generator (Sequential)      (None, 32, 32, 3)         1466115   \n",
            "                                                                 \n",
            " Discriminator (Sequential)  (None, 1)                 522497    \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1988612 (7.59 MB)\n",
            "Trainable params: 1466115 (5.59 MB)\n",
            "Non-trainable params: 522497 (1.99 MB)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training the composite model involves generating a batch worth of points in the latent space via the `generate_latent_points()` function in the previous section, and class=1 labels and calling the `train_on_batch()` function.\n",
        "\n",
        "The `train()` function below demonstrates this, although it is pretty simple as only the generator will be updated each epoch, leaving the discriminator with default model weights."
      ],
      "metadata": {
        "id": "UhewKhRr0Z2X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instead, what is required is that we first update the discriminator model with real and fake samples, then update the generator via the composite model. This requires combining elements from the `train_discriminator()` function defined in the discriminator section above and the `train_gan()` function defined above. It also requires that we enumerate over both epochs and batches within in an epoch.\n",
        "\n",
        "The complete train function for updating the discriminator model and the generator (via the composite model) is listed below. There are a few things to note in this model training function.\n",
        "\n",
        "First, the number of batches within an epoch is defined by how many times the batch size divides into the training dataset. We have a dataset size of 50K samples, so with rounding down, there are 390 batches per epoch. The discriminator model is updated twice per batch, once with real samples and once with fake samples, which is a best practice as opposed to combining the samples and performing a single update.\n",
        "\n",
        "Finally, we report the loss each batch. It is critical to keep an eye on the loss over batches. The reason for this is that a crash in the discriminator loss indicates that the generator model has started generating rubbish examples that the discriminator can easily discriminate. Monitor the discriminator loss and expect it to hover around 0.5 to 0.8 per batch. The generator loss is less critical and may hover between 0.5 and 2 or higher. **A clever programmer might even attempt to detect the crashing loss of the discriminator, halt, and then restart the training process.**"
      ],
      "metadata": {
        "id": "wCHKdvey1CxH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# train the GAN: generator and discriminator\n",
        "def train(g_model, d_model, gan_model, dataset, latent_dim, n_epochs=200, n_batch=128):\n",
        "  bat_per_epo = int(dataset.shape[0] / n_batch)\n",
        "  half_batch = int(n_batch / 2)\n",
        "  print('start GAN training')\n",
        "  # manually enumerate epochs\n",
        "  for i in range(n_epochs):\n",
        "    t = time.time()\n",
        "    # enumerate batches over the training set\n",
        "    for j in range(bat_per_epo):\n",
        "      # get randomly selected 'real' samples\n",
        "      X_real, y_real = generate_real_samples(dataset, half_batch)\n",
        "      # update discriminator model weights\n",
        "      d_loss1, _ = d_model.train_on_batch(X_real, y_real)\n",
        "      # generate 'fake' examples\n",
        "      X_fake, y_fake = generate_fake_samples(g_model, latent_dim, half_batch)\n",
        "      # update discriminator model weights\n",
        "      d_loss2, _ = d_model.train_on_batch(X_fake, y_fake)\n",
        "      # prepare points in latent space as input for the generator\n",
        "      X_gan = generate_latent_points(latent_dim, n_batch)\n",
        "      # create inverted labels for the fake samples\n",
        "      y_gan = ones((n_batch, 1))\n",
        "      # update the generator via the discriminator's error\n",
        "      g_loss, _ = gan_model.train_on_batch(X_gan, y_gan)\n",
        "      # summarize loss on this batch\n",
        "      # if j % 100 == 0:\n",
        "      #   print('>%d, %d/%d, d1=%.3f, d2=%.3f g=%.3f' % (i + 1, j + 1, bat_per_epo, d_loss1, d_loss2, g_loss))\n",
        "    # evaluate the model performance, sometimes\n",
        "    if i == 0 or (i+1) % 20 == 0:\n",
        "      print('>%d, %d/%d, d1=%.3f, d2=%.3f, g=%.3f, elapsed time:%ds' % (i + 1, j + 1, bat_per_epo, d_loss1, d_loss2, g_loss, int(time.time()-t)))\n",
        "      summarize_performance(i, g_model, d_model, dataset, latent_dim)\n",
        "\n",
        "  print('end GAN training')"
      ],
      "metadata": {
        "id": "tiAvtw781lQ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Evaluate GAN Model Performance\n",
        "Generally, there are no objective ways to evaluate the performance of a GAN model. We cannot calculate this objective error score for generated images. Instead, images must be subjectively **evaluated for quality by a human operator**. This means that we cannot know when to stop training without looking at examples of generated images. In turn, the adversarial nature of the training process means that the generator is changing after every batch, meaning that once “good enough” images can be generated, the subjective quality of the images may then begin to vary, improve, or even degrade with subsequent updates.\n",
        "\n",
        "There are three ways to handle this complex training situation.\n",
        "1. Periodically evaluate the classification accuracy of the discriminator on real and fake images.\n",
        "2. Periodically generate many images and save them to file for subjective review.\n",
        "3. Periodically save the generator model.\n",
        "\n",
        "All three of these actions can be performed at the same time for a given training epoch, such as every 10 training epochs. The result will be a saved generator model for which we have a way of subjectively assessing the quality of its output and objectively knowing how well the discriminator was fooled at the time the model was saved.\n",
        "\n",
        "Training the GAN over many epochs, such as hundreds or thousands of epochs, will result in many snapshots of the model that can be inspected, and from which specific outputs and models can be cherry-picked for later use.\n",
        "\n",
        "First, we can define a function called `summarize_performance()` that will summarize the performance of the discriminator model. It does this by retrieving a sample of real CIFAR-10 images, as well as generating the same number of fake CIFAR-10 images with the generator model, then evaluating the classification accuracy of the discriminator model on each sample, and reporting these scores."
      ],
      "metadata": {
        "id": "-QsCV4nn6ggL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluate the discriminator, plot generated images, save generator model\n",
        "def summarize_performance(epoch, g_model, d_model, dataset, latent_dim, n_samples=150):\n",
        "    # prepare real samples\n",
        "    X_real, y_real = generate_real_samples(dataset, n_samples)\n",
        "    # evaluate discriminator on real examples\n",
        "    _, acc_real = d_model.evaluate(X_real, y_real, verbose=0)\n",
        "    # prepare fake examples\n",
        "    x_fake, y_fake = generate_fake_samples(g_model, latent_dim, n_samples)\n",
        "    # evaluate discriminator on fake examples\n",
        "    _, acc_fake = d_model.evaluate(x_fake, y_fake, verbose=0)\n",
        "    # summarize discriminator performance\n",
        "    print('>Accuracy real: %.0f%%, fake: %.0f%%' % (acc_real * 100, acc_fake * 100))\n",
        "    # save plot\n",
        "    plot_img(x_fake)\n",
        "    # save models and weigths\n",
        "    model_save(g_model, 'Generator')\n",
        "    model_save(d_model, 'Discriminator')\n",
        "    model_save(gan_model, 'GAN')"
      ],
      "metadata": {
        "id": "mYAu6irh7UOm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we are evaluating the discriminator on 100 generated CIFAR-10 images, we can plot about half, or 49, as a 7 by 7 grid. The `plot_img()` function below implements this, again saving the resulting plot with a unique filename based on the epoch number."
      ],
      "metadata": {
        "id": "gNkvwNi18Jfj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eU_sjK3AMkbm"
      },
      "outputs": [],
      "source": [
        "# create a plot of generated images\n",
        "def plot_img(examples, n=8):\n",
        "    # scale from [-1,1] to [0,1]\n",
        "    examples = (examples + 1) / 2.0\n",
        "    fig = plt.figure(figsize=(9,9))\n",
        "    # plot images\n",
        "    for i in range(n * n):\n",
        "        # define subplot\n",
        "        plt.subplot(n, n, 1 + i)\n",
        "        # turn off axis\n",
        "        plt.axis('off')\n",
        "        # plot raw pixel data\n",
        "        plt.imshow(examples[i])\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The chosen configuration results in the stable training of both the generative and discriminative model. The model performance is reported every batch, including the loss of both the discriminative (d) and generative (g) models.\n",
        "\n",
        "Note: Your results may vary given the stochastic nature of the algorithm or evaluation procedure, or differences in numerical precision. Consider running the example a few times and compare the average outcome.\n",
        "\n",
        "In this case, the loss remains stable over the course of training. The discriminator loss on the real and generated examples sits around 0.5, whereas the loss for the generator trained via the discriminator sits around 1.5 for much of the training process."
      ],
      "metadata": {
        "id": "s4s0SX818e5y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# train model\n",
        "#train(g_model, d_model, gan_model, dataset, latent_dim, 150, 256)\n",
        "train(g_model, d_model, gan_model, dataset, latent_dim, 2)"
      ],
      "metadata": {
        "id": "bE8WIqdyZ-aB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# rotina para carregar os modelos das redes treinadas (modelo, pesos e bias)\n",
        "# para predições futuras sem ter que treiná-lo novamente ou para reiniciar os treinamentos\n",
        "def load_model(name):\n",
        "  mod = str(name)\n",
        "  try:\n",
        "    # Load json and create model\n",
        "    j_file = open('/content/drive/MyDrive/Colab Notebooks/temp/model'+ mod +'.json','r')\n",
        "  except:\n",
        "    print('file does not exist or drive is not open!')\n",
        "    return\n",
        "\n",
        "  load_model_json = j_file.read()\n",
        "  j_file.close()\n",
        "  loaded_model=model_from_json(load_model_json)\n",
        "  try:\n",
        "    # Load weights into new model\n",
        "    loaded_model.load_weights('/content/drive/MyDrive/Colab Notebooks/temp/weights'+ mod + '.h5')\n",
        "  except:\n",
        "    print('file does not exist or drive is not open!')\n",
        "    return\n",
        "\n",
        "  print('Loaded model ' + mod + ' from disk')\n",
        "  return loaded_model"
      ],
      "metadata": {
        "id": "gALgh_gXm-W7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# carrego os modelos treinados\n",
        "g_model = load_model('Generator')\n",
        "d_model = load_model('Discriminator')\n",
        "gan_model = load_model('GAN')"
      ],
      "metadata": {
        "id": "9vyVn7lOsws_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a6c0bdf-933e-4f94-c75e-a6a81615941a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded model Generator from disk\n",
            "Loaded model Discriminator from disk\n",
            "Loaded model GAN from disk\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def compile_discr(model):\n",
        "  # compile model discriminator\n",
        "  opt = Adam(learning_rate=0.0002, beta_1=0.5)\n",
        "  model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
        "\n",
        "def compile_gan(model):\n",
        "  # compile model gan\n",
        "  opt = Adam(learning_rate=0.0002, beta_1=0.5)\n",
        "  model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "UiNvAsIlPHM_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "compile_discr(d_model)\n",
        "compile_gan(gan_model)"
      ],
      "metadata": {
        "id": "rn7m8-dcQFoB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZQDS0_GLW2ln"
      },
      "outputs": [],
      "source": [
        "# função para gerar as imagens fake com as pedições\n",
        "def plot_generated_images(examples=50):\n",
        "  noise = np.random.normal(0, 1, size=[examples, latent_dim])\n",
        "  generated_images = g_model.predict(noise, verbose='None')\n",
        "  discr_predict = d_model.predict(generated_images, verbose='None')\n",
        "  gen_images = generated_images.reshape(examples, 32, 32, 3)\n",
        "  # scale from [-1,1] to [0,1]\n",
        "  gen_images = (gen_images + 1) / 2.0\n",
        "  dim=(10, 10)\n",
        "  plt.figure(figsize=(13, 15))\n",
        "  for i in range(gen_images.shape[0]):\n",
        "    plt.subplot(dim[0], dim[1], i+1)\n",
        "    plt.imshow(gen_images[i])\n",
        "    a=discr_predict[i][0]\n",
        "    fo = \"{:.2f}\".format(a)\n",
        "    plt.title(str(fo), size=10)\n",
        "    plt.axis('off')\n",
        "  #plt.tight_layout()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tPHBVkbCGsLz"
      },
      "outputs": [],
      "source": [
        "# # função para plotar as imagens reais com as pedições\n",
        "def plot_real_images(examples=50):\n",
        "  X_real, _ = generate_real_samples(dataset, examples)\n",
        "  discr_predict = d_model.predict(X_real, verbose='None')\n",
        "  # scale from [-1,1] to [0,1]\n",
        "  X_real = (X_real + 1) / 2.0\n",
        "  dim=(10, 10)\n",
        "  plt.figure(figsize=(13, 15))\n",
        "  for i in range(X_real.shape[0]):\n",
        "    plt.subplot(dim[0], dim[1], i+1)\n",
        "    plt.imshow(X_real[i])\n",
        "    a=discr_predict[i][0]\n",
        "    fo = \"{:.2f}\".format(a)\n",
        "    plt.title(str(fo), size=10)\n",
        "    plt.axis('off')\n",
        "  #plt.tight_layout()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plot_generated_images()"
      ],
      "metadata": {
        "id": "b6xphYbFuhN4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_real_images()"
      ],
      "metadata": {
        "id": "yg4PpmTxJVqM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Referências\n",
        "\n",
        "[1] [How to Develop a GAN to Generate CIFAR10](https://machinelearningmastery.com/how-to-develop-a-generative-adversarial-network-for-a-cifar-10-small-object-photographs-from-scratch/)\n",
        "\n",
        "[2] https://www.tensorflow.org/tutorials/generative/dcgan\n",
        "\n",
        "[3] https://pyimagesearch.com/2020/11/16/gans-with-keras-and-tensorflow/\n",
        "\n",
        "[4] https://github.com/tensorflow/gan\n",
        "\n",
        "[5] https://blog.paperspace.com/implementing-gans-in-tensorflow/\n",
        "\n",
        "[6] https://pyimagesearch.com/2020/11/16/gans-with-keras-and-tensorflow/"
      ],
      "metadata": {
        "id": "rtgdMwJSoYtd"
      }
    }
  ]
}